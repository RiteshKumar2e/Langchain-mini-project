# RAG Knowledge Assistant

A production-quality **Retrieval-Augmented Generation (RAG)** question-answering system built with **LangChain**, **FastAPI**, and **React**. Ask questions in natural language and receive grounded, cited answers from a curated knowledge base.

---

## âœ¨ Features

| Feature | Details |
|---|---|
| **RAG Pipeline** | LangChain Â· FAISS Â· Sentence-Transformers / OpenAI Embeddings |
| **LLM Providers** | Groq (free, fast) or OpenAI â€” swappable via `.env` |
| **Source Citations** | Every answer cites the source documents it was retrieved from |
| **Follow-up Questions** | Conversation history is maintained for multi-turn dialogue |
| **History Persistence** | All interactions logged to `history.jsonl`, viewable in the UI |
| **Light Theme UI** | Clean React app with Vite, CSS Modules, and react-markdown |
| **Modular Backend** | Config â†’ Embeddings â†’ Ingestor â†’ RAG Chain â†’ Router â†’ FastAPI |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RAG PIPELINE                                     â”‚
â”‚                                                                          â”‚
â”‚  INDEXING (offline)          RETRIEVAL + GENERATION (online)             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
â”‚  documents/                  User Question                               â”‚
â”‚      â†“                           â†“                                       â”‚
â”‚  DirectoryLoader             Embed Question                              â”‚
â”‚      â†“                           â†“                                       â”‚
â”‚  RecursiveCharacter          FAISS Similarity Search                     â”‚
â”‚  TextSplitter                    â†“                                       â”‚
â”‚      â†“                       Top-K Chunks                                â”‚
â”‚  Embeddings                      â†“                                       â”‚
â”‚  (MiniLM / OpenAI)           RAG Prompt (context + question)            â”‚
â”‚      â†“                           â†“                                       â”‚
â”‚  FAISS VectorStore  â”€â”€â”€â”€â”€â”€â”€â”€â–º LLM (Groq / OpenAI)                        â”‚
â”‚      â†“                           â†“                                       â”‚
â”‚  Saved to disk               Answer + Sources                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Folder Structure

```
Langchain mini project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ documents/              # 10+ knowledge-base Markdown files
â”‚   â”‚   â”œâ”€â”€ python_intro.md
â”‚   â”‚   â”œâ”€â”€ machine_learning_basics.md
â”‚   â”‚   â”œâ”€â”€ langchain_overview.md
â”‚   â”‚   â”œâ”€â”€ artificial_intelligence.md
â”‚   â”‚   â”œâ”€â”€ deep_learning.md
â”‚   â”‚   â”œâ”€â”€ nlp_overview.md
â”‚   â”‚   â”œâ”€â”€ vector_databases.md
â”‚   â”‚   â”œâ”€â”€ rag_explained.md
â”‚   â”‚   â”œâ”€â”€ transformers_and_attention.md
â”‚   â”‚   â”œâ”€â”€ fastapi_overview.md
â”‚   â”‚   â”œâ”€â”€ git_version_control.md
â”‚   â”‚   â”œâ”€â”€ docker_containerization.md
â”‚   â”‚   â”œâ”€â”€ react_overview.md
â”‚   â”‚   â””â”€â”€ cloud_computing.md
â”‚   â”œâ”€â”€ vector_store/           # Auto-generated FAISS index (gitignored)
â”‚   â”œâ”€â”€ config.py               # All settings via pydantic-settings
â”‚   â”œâ”€â”€ logger.py               # Shared logging setup
â”‚   â”œâ”€â”€ embeddings.py           # Embedding model factory (cached)
â”‚   â”œâ”€â”€ ingestor.py             # Document load â†’ split â†’ embed â†’ store
â”‚   â”œâ”€â”€ rag_pipeline.py         # LLM factory + RetrievalQA chain + ask()
â”‚   â”œâ”€â”€ history.py              # JSONL history persistence
â”‚   â”œâ”€â”€ schemas.py              # Pydantic request/response models
â”‚   â”œâ”€â”€ router.py               # FastAPI route handlers
â”‚   â”œâ”€â”€ main.py                 # App factory, CORS, lifespan
â”‚   â”œâ”€â”€ history.jsonl           # Auto-generated per-run (gitignored)
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â””â”€â”€ frontend/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”œâ”€â”€ Header.jsx / .module.css
    â”‚   â”‚   â”œâ”€â”€ MessageBubble.jsx / .module.css
    â”‚   â”‚   â”œâ”€â”€ ChatInput.jsx / .module.css
    â”‚   â”‚   â””â”€â”€ HistoryPanel.jsx / .module.css
    â”‚   â”œâ”€â”€ hooks/
    â”‚   â”‚   â””â”€â”€ useChat.js      # Chat state + follow-up history
    â”‚   â”œâ”€â”€ api.js              # Backend API client
    â”‚   â”œâ”€â”€ App.jsx / .module.css
    â”‚   â”œâ”€â”€ index.css           # Global design system
    â”‚   â””â”€â”€ main.jsx
    â”œâ”€â”€ index.html
    â”œâ”€â”€ vite.config.js
    â””â”€â”€ package.json
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Node.js 18+
- A **Groq API key** (free at [console.groq.com](https://console.groq.com)) **OR** an OpenAI key

---

### 1 â€” Backend Setup

```bash
cd backend

# Create and activate virtual environment
python -m venv venv
venv\Scripts\activate          # Windows
# source venv/bin/activate     # macOS/Linux

# Install dependencies
pip install -r requirements.txt

# Configure environment
copy .env.example .env         # Windows
# cp .env.example .env         # macOS/Linux
```

Edit `.env` and set your API key:

```env
# For Groq (free, recommended)
GROQ_API_KEY=gsk_...
LLM_PROVIDER=groq

# For OpenAI
OPENAI_API_KEY=sk-...
LLM_PROVIDER=openai

# Embeddings (huggingface = free & local, no key needed)
EMBEDDING_PROVIDER=huggingface
```

### 2 â€” Build the Knowledge Base Index

```bash
python ingestor.py
# â†’ Loads 14 documents, splits into ~300 chunks, builds FAISS index
# â†’ Saves to ./vector_store/
```

To force a rebuild if you add new documents:

```bash
python ingestor.py --force
```

### 3 â€” Start the Backend API

```bash
uvicorn main:app --reload --port 8000
```

- API docs: http://localhost:8000/docs
- Health check: http://localhost:8000/api/health

---

### 4 â€” Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

Open http://localhost:5173 in your browser.

---

## ğŸ”Œ API Reference

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/ask` | Submit a question, receive answer + sources |
| `GET`  | `/api/health` | Check readiness |
| `GET`  | `/api/history` | Retrieve recent interactions |
| `POST` | `/api/history/clear` | Clear all history |
| `POST` | `/api/ingest` | Rebuild the FAISS index |

### Example Request

```bash
curl -X POST http://localhost:8000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "What is RAG and how does it work?"}'
```

### Example Response

```json
{
  "question": "What is RAG and how does it work?",
  "answer": "RAG (Retrieval-Augmented Generation) is an AI architecture that enhances LLM responses...",
  "sources": [
    {
      "filename": "rag_explained.md",
      "snippet": "RAG is a technique that combines retrieval with generation...",
      "start_index": 0
    }
  ]
}
```

---

## ğŸ“š Knowledge Base Documents

The assistant is pre-loaded with 14 curated documents covering:

- Python programming fundamentals
- Machine learning concepts
- Deep learning and neural networks
- Natural language processing
- Transformer architecture and attention
- RAG (Retrieval-Augmented Generation)
- LangChain framework
- Vector databases (FAISS, Chroma, Pinecone, etc.)
- FastAPI
- React.js
- Git & version control
- Docker & containerization
- Cloud computing
- Artificial intelligence overview

---

## ğŸ”§ Configuration Reference

All settings live in `backend/.env`:

| Variable | Default | Description |
|----------|---------|-------------|
| `LLM_PROVIDER` | `groq` | `groq` or `openai` |
| `GROQ_API_KEY` | â€” | Required if provider=groq |
| `OPENAI_API_KEY` | â€” | Required if provider=openai |
| `GROQ_MODEL` | `llama-3.3-70b-versatile` | Groq model name |
| `OPENAI_MODEL` | `gpt-4o-mini` | OpenAI model name |
| `EMBEDDING_PROVIDER` | `huggingface` | `huggingface` or `openai` |
| `CHUNK_SIZE` | `800` | Characters per document chunk |
| `CHUNK_OVERLAP` | `100` | Overlap between chunks |
| `RETRIEVAL_K` | `5` | Documents to retrieve per query |

---

## ğŸ§ª Adding New Documents

1. Drop any `.md` or `.txt` file into `backend/documents/`
2. Rebuild the index: `python ingestor.py --force`
3. Or use the API: `POST /api/ingest`

No code changes required.

---

## ğŸ“„ License

This project is for evaluation purposes only.
